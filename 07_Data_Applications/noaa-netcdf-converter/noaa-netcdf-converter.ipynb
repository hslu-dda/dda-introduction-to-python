{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOAA Sea Surface Temperature (SST) Data Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023.12.0\n",
      "1.7.2\n",
      "{'netcdf4': <NetCDF4BackendEntrypoint>\n",
      "  Open netCDF (.nc, .nc4 and .cdf) and most HDF5 files using netCDF4 in Xarray\n",
      "  Learn more at https://docs.xarray.dev/en/stable/generated/xarray.backends.NetCDF4BackendEntrypoint.html, 'h5netcdf': <H5netcdfBackendEntrypoint>\n",
      "  Open netCDF (.nc, .nc4 and .cdf) and most HDF5 files using h5netcdf in Xarray\n",
      "  Learn more at https://docs.xarray.dev/en/stable/generated/xarray.backends.H5netcdfBackendEntrypoint.html, 'scipy': <ScipyBackendEntrypoint>\n",
      "  Open netCDF files (.nc, .nc4, .cdf and .gz) using scipy in Xarray\n",
      "  Learn more at https://docs.xarray.dev/en/stable/generated/xarray.backends.ScipyBackendEntrypoint.html, 'store': <StoreBackendEntrypoint>\n",
      "  Open AbstractDataStore instances in Xarray\n",
      "  Learn more at https://docs.xarray.dev/en/stable/generated/xarray.backends.StoreBackendEntrypoint.html}\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "print(xr.__version__) # should output 2023.12.0\n",
    "\n",
    "import netCDF4\n",
    "print(netCDF4.__version__) # should output 1.7.2\n",
    "\n",
    "from xarray.backends import list_engines\n",
    "print(list_engines()) # check that 'netcdf4' or 'nc4' is part of the engine\n",
    "\n",
    "# import pandas and altair for further processing & visualization\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "alt.data_transformers.enable(\"vegafusion\") ## for big data sets\n",
    "\n",
    "import os # to read many files from the operating system\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Exploring the Data Structure\n",
    "\n",
    "First we open one example file to get to know the netCDF4 format a bit better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:  (time: 1, zlev: 1, lat: 720, lon: 1440)\n",
      "Coordinates:\n",
      "  * time     (time) datetime64[ns] 2025-01-01T12:00:16.364011520\n",
      "  * zlev     (zlev) float32 0.0\n",
      "  * lat      (lat) float32 -89.88 -89.62 -89.38 -89.12 ... 89.38 89.62 89.88\n",
      "  * lon      (lon) float32 0.125 0.375 0.625 0.875 ... 359.1 359.4 359.6 359.9\n",
      "Data variables:\n",
      "    sst      (time, zlev, lat, lon) float32 ...\n",
      "    anom     (time, zlev, lat, lon) float32 ...\n",
      "    err      (time, zlev, lat, lon) float32 ...\n",
      "    ice      (time, zlev, lat, lon) float32 ...\n",
      "Attributes: (12/37)\n",
      "    Conventions:                CF-1.6, ACDD-1.3\n",
      "    title:                      NOAA/NCEI 1/4 Degree Daily Optimum Interpolat...\n",
      "    references:                 Reynolds, et al.(2007) Daily High-Resolution-...\n",
      "    source:                     ICOADS, NCEP_GTS, GSFC_ICE, NCEP_ICE, Pathfin...\n",
      "    id:                         oisst-avhrr-v02r01.20250101.nc\n",
      "    naming_authority:           gov.noaa.ncei\n",
      "    ...                         ...\n",
      "    time_coverage_start:        2025-01-01T00:00:00Z\n",
      "    time_coverage_end:          2025-01-01T23:59:59Z\n",
      "    metadata_link:              https://doi.org/10.25921/RE9P-PT57\n",
      "    ncei_template_version:      NCEI_NetCDF_Grid_Template_v2.0\n",
      "    comment:                    Data was converted from NetCDF-3 to NetCDF-4 ...\n",
      "    sensor:                     Thermometer, AVHRR\n"
     ]
    }
   ],
   "source": [
    "# load one file and print the data\n",
    "ds = xr.open_dataset(\"./data/oisst-avhrr-v02r01.20250101_example.nc\", engine='netcdf4')\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SST (sea surface temperature) is 4-dimensional:\n",
    "- time: 1 time point (for now, just the 20250101 file)\n",
    "- zlev: 1 depth level (surface = 0m)\n",
    "- lat and lon: your spatial grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataArray 'sst' (time: 1, zlev: 1, lat: 720, lon: 1440)>\n",
      "[1036800 values with dtype=float32]\n",
      "Coordinates:\n",
      "  * time     (time) datetime64[ns] 2023-01-01T11:59:54.563395584\n",
      "  * zlev     (zlev) float32 0.0\n",
      "  * lat      (lat) float32 -89.88 -89.62 -89.38 -89.12 ... 89.38 89.62 89.88\n",
      "  * lon      (lon) float32 0.125 0.375 0.625 0.875 ... 359.1 359.4 359.6 359.9\n",
      "Attributes:\n",
      "    long_name:  Daily sea surface temperature\n",
      "    units:      Celsius\n",
      "    valid_min:  -300\n",
      "    valid_max:  4500\n"
     ]
    }
   ],
   "source": [
    "# get the sst data section and print it\n",
    "sst = ds['sst']\n",
    "print(sst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the SST for a specific region (bounding box)\n",
    "\n",
    "To focus on a region like the North Atlantic, you can slice the dataset like this:\n",
    "\n",
    "```python\n",
    "northAtlantic = ds.sst.sel(lat=slice(30, 60), lon=slice(-80, 0))\n",
    "```\n",
    "\n",
    "Attention: Many climate and ocean datasets, including NOAA’s OISST, store longitude in the range [0, 360], instead of the more intuitive -180° to 180° format (where negative = west).\n",
    "For example:\n",
    "\n",
    "- 280°E is equivalent to -80°W.\n",
    "- 359.9°E is just shy of 0°.\n",
    "\n",
    "Therefore the northAtlantic would be: `lat=slice(30, 60), lon=slice(280, 360)`. You can check the format with `print(ds.lon.values)`\n",
    "\n",
    "To handle this in Xarray, you can shift longitudes to [0, 360] like this: `ds = ds.assign_coords(lon=(ds.lon % 360))` (ChatGPT untested).\n",
    "\n",
    "Ideally use QGIS or some tool to define your bounding box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataArray 'sst' (time: 1, zlev: 1, lat: 120, lon: 320)>\n",
      "[38400 values with dtype=float32]\n",
      "Coordinates:\n",
      "  * time     (time) datetime64[ns] 2025-01-01T12:00:16.364011520\n",
      "  * zlev     (zlev) float32 0.0\n",
      "  * lat      (lat) float32 30.12 30.38 30.62 30.88 ... 59.12 59.38 59.62 59.88\n",
      "  * lon      (lon) float32 280.1 280.4 280.6 280.9 ... 359.1 359.4 359.6 359.9\n",
      "Attributes:\n",
      "    long_name:  Daily sea surface temperature\n",
      "    units:      Celsius\n",
      "    valid_min:  -300\n",
      "    valid_max:  4500\n"
     ]
    }
   ],
   "source": [
    "northAtlantic = ds.sst.sel(lat=slice(30, 60), lon=slice(280, 360))\n",
    "print(northAtlantic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the actual temperature values you need to call the following. That output is the sea surface temperature (SST) in Celsius.\n",
    "\n",
    "- Warm water (25°C) near the southern end of your region.\n",
    "- Colder water (around 8-9°C) closer to 60°N.\n",
    "- Some nan values (no data or land areas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[25.82       25.849998   25.46       ...         nan         nan\n",
      "            nan]\n",
      "   [25.699999   25.869999   25.599998   ...         nan         nan\n",
      "            nan]\n",
      "   [25.25       25.789999   25.75       ...         nan         nan\n",
      "            nan]\n",
      "   ...\n",
      "   [-0.5        -0.17999999  0.06       ...  8.74        8.67\n",
      "     8.54      ]\n",
      "   [-0.74       -0.42999998 -0.14       ...  8.84        8.82\n",
      "     8.72      ]\n",
      "   [-0.89       -0.56       -0.21       ...  8.9         8.94\n",
      "     8.9       ]]]]\n"
     ]
    }
   ],
   "source": [
    "print(northAtlantic.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the Values into a Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "df = northAtlantic.to_dataframe().reset_index()\n",
    "df.head()\n",
    "\n",
    "# Drop missing (NaN) values if needed (not tested, ChatGPT)\n",
    "# df = df.dropna(subset=['sst'])\n",
    "\n",
    "# close the dataset to free memory\n",
    "ds.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Scaling Up\n",
    "\n",
    "So far so good. However, the matter is more complex. The files from the [NOAA directory](https://www.ncei.noaa.gov/data/sea-surface-temperature-optimum-interpolation/v2.1/access/avhrr/) are for each day of the month for each year (1981 to 2025). We need to calculate a yearly average for each year. For each month of the year we can take the first day and then calculate the average of the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/2023/oisst-avhrr-v02r01.20230101.nc\n",
      "./data/2023/oisst-avhrr-v02r01.20230201.nc\n",
      "./data/2023/oisst-avhrr-v02r01.20230301.nc\n",
      "./data/2023/oisst-avhrr-v02r01.20230401.nc\n",
      "./data/2023/oisst-avhrr-v02r01.20230501.nc\n",
      "./data/2023/oisst-avhrr-v02r01.20230601.nc\n",
      "./data/2023/oisst-avhrr-v02r01.20230701.nc\n",
      "./data/2023/oisst-avhrr-v02r01.20230801.nc\n",
      "./data/2023/oisst-avhrr-v02r01.20230901.nc\n",
      "./data/2023/oisst-avhrr-v02r01.20231001.nc\n",
      "./data/2023/oisst-avhrr-v02r01.20231101.nc\n",
      "./data/2023/oisst-avhrr-v02r01.20231201.nc\n"
     ]
    }
   ],
   "source": [
    "# set the year you want to calculate an combined file, change this for the next year!\n",
    "set_year = '2023'\n",
    "\n",
    "# create the folder path automatically\n",
    "set_dir = './data/' + set_year + '/'\n",
    "\n",
    "# List files and sort by date embedded in filename\n",
    "files_2023 = sorted(\n",
    "    [os.path.join(set_dir, file) for file in os.listdir(set_dir) if file.endswith('.nc')],\n",
    "    key=lambda x: x.split('.')[-2]  # This grabs the YYYYMMDD part right before '.nc'\n",
    ")\n",
    "\n",
    "# Optional: print to verify order\n",
    "for f in files_2023:\n",
    "    print(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all files from one year loaded, we can iterate over each file using a for loop and save each files data into a Pandas Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "460800\n",
      "                           time  zlev     lat      lon        sst  year\n",
      "0 2023-01-01 11:59:54.563395584   0.0  30.125  280.125  25.709999  2023\n",
      "1 2023-01-01 11:59:54.563395584   0.0  30.125  280.375  25.830000  2023\n",
      "2 2023-01-01 11:59:54.563395584   0.0  30.125  280.625  25.730000  2023\n",
      "38400\n",
      "       year     lat      lon        sst\n",
      "0      2023  30.125  280.125  27.776667\n",
      "1      2023  30.125  280.375  27.776667\n",
      "2      2023  30.125  280.625  27.362497\n",
      "3      2023  30.125  280.875  26.847498\n",
      "4      2023  30.125  281.125  26.438334\n",
      "...     ...     ...      ...        ...\n",
      "38395  2023  59.875  358.875  10.227500\n",
      "38396  2023  59.875  359.125  10.373333\n",
      "38397  2023  59.875  359.375  10.501666\n",
      "38398  2023  59.875  359.625  10.549167\n",
      "38399  2023  59.875  359.875  10.521667\n",
      "\n",
      "[38400 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store the data of the year\n",
    "# for each file we open, we will save a data frame into this\n",
    "# it will be a list of data frames\n",
    "data_list = []\n",
    "\n",
    "# Iterate over each file you loaded before\n",
    "for file in files_2023:\n",
    "    # Open the current file/dataset\n",
    "    current_ds = xr.open_dataset(file)\n",
    "\n",
    "    # Security check if 'sst' is in the file\n",
    "    if 'sst' not in current_ds.variables:\n",
    "        print(f\"Warning: 'sst' not found in {file}\")\n",
    "        continue  # Skip file if variable is missing\n",
    "\n",
    "    # select the area (north american ocean)\n",
    "    area_select = current_ds.sst.sel(lat=slice(30, 60), lon=slice(280, 360))\n",
    "\n",
    "    # get the values info a data frame\n",
    "    new_df = area_select.to_dataframe().reset_index()\n",
    "\n",
    "    # Add year column to keep track of the current year\n",
    "    new_df['year'] = set_year\n",
    "\n",
    "    # append the dataframe with the data of the current file to the combined list\n",
    "    data_list.append(new_df)\n",
    "    \n",
    "    # Close the dataset to free up memory\n",
    "    current_ds.close()\n",
    "\n",
    "\n",
    "# Now we have one big list of dataframes, one frame for each file in the year directory\n",
    "# now we combine all of them into one dataframe for further processing\n",
    "final_df = pd.concat(data_list, ignore_index=True)\n",
    "\n",
    "# we can see the lenght and first entries of this large dataframe\n",
    "print(len(final_df))\n",
    "# Show the first few rows of the dataframe\n",
    "print(final_df.head(3))\n",
    "\n",
    "# Now we need to create a mean for each position (long/lat), as every position is now multiple times in the frame (one time for each file)\n",
    "# We can do this by grouping the entries of the data frame by year, lat and log and then calculate the mean using Pandas\n",
    "mean_sst_df = final_df.groupby(['year', 'lat', 'lon'])['sst'].mean().reset_index()\n",
    "\n",
    "# yaay!\n",
    "print(len(mean_sst_df))\n",
    "print(mean_sst_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './export/'\n",
    "\n",
    "final_df.to_csv(f'{output_dir}north_atlantic_{set_year}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
